Our project aims to develop a machine learning model for the detection of toxic comments on social media platforms, specifically targeting instances of harassment, hate speech, and abusive language. Getting two publicly available datasets from Kaggle, this study addresses the pressing need for advanced moderation tools to foster safer and more positive user experiences in online communities. Grounded in natural language processing (NLP) methodologies, the project builds on prior research, including influential contributions from Googleâ€™s Jigsaw team and recent innovations in BERT-based text classification models, to enhance both the accuracy and scalability of toxicity detection. By advancing these capabilities, this study contributes to the broader objective of creating a more respectful and inclusive digital landscape.
